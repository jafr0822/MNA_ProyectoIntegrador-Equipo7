{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jafr0822/MNA_ProyectoIntegrador-Equipo7/blob/main/Avance5_7_Equipo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jioLoDCGmvb"
      },
      "source": [
        "# **Proyecto Integrador**\n",
        "\n",
        "\n",
        "## **Tecnológico de Monterrey**\n",
        "### **Maestría en Inteligencia Artificial Aplicada (MNA)**\n",
        "#### Avance 5\n",
        "#### Equipo 7\n",
        "\n",
        "\n",
        "* Jorge Arturo Federico Rivera – A01250724\n",
        "* Marco Antonio Vázquez Morales – A01793704\n",
        "* Alejandro Jesús Vázquez Navarro - A01793146"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2ec6ClBSlp5"
      },
      "source": [
        "# Índice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfSuRoYTGvGO"
      },
      "source": [
        "## I    Introducción\n",
        "## II   EDA\n",
        "\n",
        "1.   Estructura de datos\n",
        "2.   Funciones Ad-hoc\n",
        "3.   Dendograma\n",
        "4.   Densidad\n",
        "5.   Preprocesamiento\n",
        "6.   Imputación de datos\n",
        "7.   Imputación para variables\n",
        "8.   Evaluación de Porosidad\n",
        "9.   Análisis Bivariado\n",
        "10.  Detección de Outliers\n",
        "11.  Selección de variable dependiente\n",
        "\n",
        "## III  Ingenieria de Características\n",
        "\n",
        "1.   Construcción\n",
        "2.   Normalización\n",
        "3.   Selección y Extracción\n",
        "\n",
        "## IV   Baseline\n",
        "\n",
        "1.   Algoritmo\n",
        "2.   Características importantes\n",
        "3.   Sub y Sobreajuste\n",
        "4.   Métricas\n",
        "5.   Desempeño\n",
        "\n",
        "## V    Modelos Alternativos\n",
        "\n",
        "1. Preparación\n",
        "2. Comparativa\n",
        "3. Ajuste Fino\n",
        "\n",
        "## VI. Modelos de Ensamble\n",
        "\n",
        "\n",
        "1.   Homogéneos y Heterogéneos\n",
        "2.   Comparativa de Modelo Individual vs Ensamble\n",
        "3.   Selección de Modelo Final\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WMpmM2nRnjB"
      },
      "source": [
        "### Este trabajo tiene como finalidad crear un modelo que pueda predecir el riesgo de sufrir alguna afectación a la mujer o su feto en el proceso de embarazo, tomando como base los datos clínicos, metabólicos, genéticos y nutricionales. Se trata de encontrar una variable dependiente que permita detectar las emfermedades a tiempo para poder reducir el impacto negativo que puediera tener en uno o ambos casos.\n",
        "\n",
        "### En primer termino se enlistan las variabales que se van a utilizar, en esta étapa identificamos 99 en donde las que estan en color naranja representan las posibles variables dependendientes, las que estan en azul son las variables explicativas, las que estan en color rojo serían las variables de salida.\n",
        "\n",
        "### Posteriormente realizamos una revisión a los datos, luego quitamos las columnas que no serán útiles para después realizar un análisis exploratorio de los datos utilizando ydata-profilling y Sweetviz.\n",
        "\n",
        "### Debido al número extenso de variables (96) generamos reportes estáticos en html para su mejor visualización.\n",
        "\n",
        "### Estos pueden consultarse en estas ligas:\n",
        "- [Reporte mínimo exploratorio](https://github.com/jafr0822/MNA_ProyectoIntegrador-Equipo7/blob/8a9e4300831ede90791258bd5b58acd44c456537/data/reports/profile1_min_report.html)\n",
        "\n",
        "- [Reporte completo exploratorio](https://github.com/jafr0822/MNA_ProyectoIntegrador-Equipo7/blob/8a9e4300831ede90791258bd5b58acd44c456537/data/reports/profile1_exp_report.html)\n",
        "\n",
        "- [Reporte realizado con sweetViz](https://github.com/jafr0822/MNA_ProyectoIntegrador-Equipo7/blob/8a9e4300831ede90791258bd5b58acd44c456537/data/reports/sv_report1.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLf9uTxEUIpm"
      },
      "source": [
        "# II EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e44AGIWKSX5_"
      },
      "source": [
        "## Estructura de datos\n",
        "\n",
        "Despúes de una revisión con la parte médica, definimos las siguientes variables como la estrucutura base para la generación de los modelos predicitivos.\n",
        "A continuación se listan y describen las variables que serán utilizadas para este proyecto.\n",
        "\n",
        "Las variables en rojo serán consolidadas para la creación de la variable dependiente llamada `IndexMorbilidad`\n",
        "\n",
        "La variable dependiente candidata (en marrón) era `sdg_parto`, se descartó debido a que solo está presente en el 50.5%  de las observaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj-JMp_IVQET"
      },
      "source": [
        "## Instalamos librerias para colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61eF7XxnECZ7",
        "outputId": "6772d206-e0b9-4211-b2e0-633cfaee47ac"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>.output { max-width:100% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import warnings\n",
        "from IPython.display import display, HTML\n",
        "import pandas as pd\n",
        "\n",
        "display(HTML(\"<style>.output { max-width:100% !important; }</style>\"))\n",
        "warnings.filterwarnings(\"ignore\", category=Warning)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcItRUTTVMdV"
      },
      "outputs": [],
      "source": [
        "#%pip install matplotlib-venn missingno sweetviz ydata-profiling\n",
        "#%pip install --upgrade Pillow\n",
        "#%pip install --upgrade scikit-learn\n",
        "#%pip install git+https://github.com/MIDASverse/MIDASpy.git\n",
        "\n",
        "import importlib\n",
        "\n",
        "# List of libraries to install\n",
        "libraries_to_install = [\n",
        "    (\"matplotlib_venn\", \"matplotlib_venn\"),\n",
        "    (\"missingno\", \"missingno\"),\n",
        "    (\"sweetviz\", \"sweetviz\"),\n",
        "    (\"ydata_profiling\", \"pandas_profiling\"),\n",
        "    (\"Pillow\", \"PIL\"),\n",
        "    (\"scikit-learn\", \"sklearn\"),\n",
        "]\n",
        "\n",
        "for package, module in libraries_to_install:\n",
        "    try:\n",
        "        importlib.import_module(module)\n",
        "        print(f\"{module} is already installed.\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {module}...\")\n",
        "        %pip install {package}\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import sweetviz as sv\n",
        "import numpy as np\n",
        "from ydata_profiling import ProfileReport\n",
        "import seaborn as sns\n",
        "\n",
        "# CREATE REPORTS\n",
        "_BLN_REPORTS = False\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, learning_curve\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCu2HK4sWrlF"
      },
      "outputs": [],
      "source": [
        "# Creamos una función para calcular y graficar la porosidad del dataframe\n",
        "def getPorosidad(df):\n",
        "\n",
        "  porosidad = df.isnull().sum() / len(df)\n",
        "\n",
        "  porosidad_df = pd.DataFrame({\n",
        "      'Caracteristica': porosidad.index,\n",
        "      'Porosidad': porosidad.values\n",
        "  })\n",
        "\n",
        "  # Ordenar el DataFrame por porosidad de mayor a menor\n",
        "  porosidad_df = porosidad_df.sort_values(by='Porosidad', ascending=True)\n",
        "  # Crear la gráfica de barras\n",
        "  plt.figure(figsize=(6, 14))\n",
        "  variables_interes = ['sdg_parto', 'preeclampsia', 'polihidramnios', 'hidramnios', 'infeccion_gesta', 'malformaciones_rn', 'aborto_rn', 'obito_rn', 'mnt_rn']\n",
        "\n",
        "  colors = ['skyblue' if x not in variables_interes else 'orange' for x in porosidad_df['Caracteristica']]\n",
        "\n",
        "  bars = plt.barh(porosidad_df['Caracteristica'], porosidad_df['Porosidad'], color=colors)  # Use barh for horizontal bars\n",
        "\n",
        "  # Add value at the end of each bar\n",
        "  for bar, value in zip(bars, porosidad_df['Porosidad']):\n",
        "      plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, f\"{value:.2%}\", va='center', ha='left')\n",
        "\n",
        "  plt.xlabel('% Porosidad')\n",
        "  plt.ylabel('Caracteristica')\n",
        "  plt.title('Porosidad por Columna')\n",
        "  plt.xticks(rotation=90, ha='right')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def getPorosidadQuintiles(df):\n",
        "\n",
        "    porosidad = df.isnull().sum() / len(df)\n",
        "    porosidad_df = pd.DataFrame({\n",
        "        'Caracteristica': porosidad.index,\n",
        "        'Porosidad': porosidad.values\n",
        "    })\n",
        "\n",
        "    # Ordenar el DataFrame por porosidad de mayor a menor\n",
        "    porosidad_df = porosidad_df.sort_values(by='Porosidad', ascending=True)\n",
        "\n",
        "    # Crear la gráfica de barras\n",
        "    plt.figure(figsize=(6, 14))\n",
        "\n",
        "    variables_interes = ['sdg_parto', 'preeclampsia', 'polihidramnios', 'hidramnios', 'infeccion_gesta', 'malformaciones_rn', 'aborto_rn', 'obito_rn', 'mnt_rn']\n",
        "    colors = ['skyblue' if x not in variables_interes else 'orange' for x in porosidad_df['Caracteristica']]\n",
        "\n",
        "    bars = plt.barh(porosidad_df['Caracteristica'], porosidad_df['Porosidad'], color=colors)  # Use barh for horizontal bars\n",
        "\n",
        "    # Add value at the end of each bar\n",
        "    for bar, value in zip(bars, porosidad_df['Porosidad']):\n",
        "        plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, f\"{value:.2%}\", va='center', ha='left')\n",
        "\n",
        "    # Añadir marcadores para los quintiles\n",
        "    quintiles = porosidad_df['Porosidad'].quantile([0.2, 0.4, 0.6, 0.8])\n",
        "    for quintile in quintiles:\n",
        "        plt.axvline(x=quintile, color='red', linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.xlabel('% Porosidad')\n",
        "    plt.ylabel('Caracteristica')\n",
        "    plt.title('Porosidad por Columna')\n",
        "    plt.xticks(rotation=90, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#####################################################\n",
        "\n",
        "def getVariablesUltimoQuintil(df):\n",
        "    porosidad = df.isnull().sum() / len(df)\n",
        "    porosidad_df = pd.DataFrame({\n",
        "        'Caracteristica': porosidad.index,\n",
        "        'Porosidad': porosidad.values\n",
        "    })\n",
        "\n",
        "    # Ordenar el DataFrame por porosidad de mayor a menor\n",
        "    porosidad_df = porosidad_df.sort_values(by='Porosidad', ascending=True)\n",
        "\n",
        "    # Calcular los quintiles de la porosidad\n",
        "    quintiles = porosidad_df['Porosidad'].quantile([0.2, 0.4, 0.6, 0.8])\n",
        "\n",
        "    # Filtrar características que están en el último quintil\n",
        "    ult_quintil = porosidad_df[porosidad_df['Porosidad'] > quintiles[0.8]]\n",
        "\n",
        "    return ult_quintil['Caracteristica'].tolist()\n",
        "\n",
        "########################################################\n",
        "\n",
        "# Creamos una función para separar las variables categóricas, numéricas\n",
        "def extract_column_types(df):\n",
        "    numeric_cols = []\n",
        "    string_cols = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['int64', 'float64']:\n",
        "            numeric_cols.append(col)\n",
        "        elif df[col].dtype == 'object':\n",
        "            string_cols.append(col)\n",
        "\n",
        "    return numeric_cols, string_cols\n",
        "\n",
        "\n",
        "########################################################\n",
        "\n",
        "def plot_distribution(df):\n",
        "    # Extraer columnas numéricas\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "    # Calcular el número de filas y columnas para el mosaico\n",
        "    num_cols = len(numeric_cols)\n",
        "    num_rows = (num_cols + 1) // 2  # Redondeo hacia arriba\n",
        "\n",
        "    # Crear una cuadrícula de subgráficos\n",
        "    fig, axes = plt.subplots(num_rows, 2, figsize=(12, num_rows * 6))\n",
        "    axes = axes.flatten()  # Convertir la matriz de ejes en una lista plana\n",
        "\n",
        "    # Iterar sobre cada columna numérica y crear una gráfica de distribución\n",
        "    for i, col in enumerate(numeric_cols):\n",
        "        sns.histplot(df[col], kde=True, color='skyblue', ax=axes[i])\n",
        "        axes[i].set_title(f'Distribución de {col}')\n",
        "        axes[i].set_xlabel(col)\n",
        "        axes[i].set_ylabel('Frecuencia')\n",
        "\n",
        "    # Ocultar ejes sobrantes\n",
        "    for j in range(i+1, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#######################################\n",
        "\n",
        "def get_column_types(df):\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    string_cols = df.select_dtypes(include=[object]).columns.tolist()\n",
        "    #binary_cols = [col for col in df.columns if col not in numeric_cols and col not in string_cols]\n",
        "    return numeric_cols, string_cols\n",
        "\n",
        "#######################################\n",
        "\n",
        "def identify_binary(data):\n",
        "\n",
        "  binary_cols = []\n",
        "  for col in data.columns:\n",
        "    unique_values = data[col].dropna().unique()\n",
        "    #print(f'Col name {col} unique values {unique_values}')\n",
        "\n",
        "    if len(unique_values) == 3 and (set(unique_values) == {\"\", 0.0, 1.0}):  # Check for 0, 1 or \"\", 1\n",
        "\n",
        "      binary_cols.append(col)\n",
        "  return binary_cols\n",
        "\n",
        "\n",
        "######################################\n",
        "\n",
        "def detect_outliers_iqr(df):\n",
        "    outliers = []\n",
        "    for col in df.columns:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        col_outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index\n",
        "        outliers.extend(col_outliers)\n",
        "    return list(set(outliers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JbQ5JB6XJa_"
      },
      "source": [
        "## Lectura de datos desde excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uil027DwXWrM"
      },
      "outputs": [],
      "source": [
        "# Se agrega esta configuración para mostrar todas las variables en el método describe.().T\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "\n",
        "cols_to_include = [\n",
        "    \"id_gdg\",\n",
        "\"biopsias\",\n",
        "\"estado\",\n",
        "\"edad_dx\",\n",
        "\"imc_h\",\n",
        "\"obs_h\",\n",
        "\"peso_1\",\n",
        "\"peso_2\",\n",
        "\"peso_3\",\n",
        "\"sdg_dx\",\n",
        "\"gluc_dx_0\",\n",
        "\"gluc_dx_60\",\n",
        "\"gluc_dx_120\",\n",
        "\"gluc_dx_180\",\n",
        "\"dx_inicio\",\n",
        "\"auc\",\n",
        "\"ichos_pregesta\",\n",
        "\"hta_pregesta\",\n",
        "\"sop\",\n",
        "\"hipotiroidismo\",\n",
        "\"hipertiroidismo\",\n",
        "\"consumo_alcohol\",\n",
        "\"consumo_tabaco\",\n",
        "\"anticonceptivo\",\n",
        "\"ah_dm\",\n",
        "\"ah_hta\",\n",
        "\"ah_ob\",\n",
        "\"ah_dg\",\n",
        "\"no_gestas\",\n",
        "\"ant_dg\",\n",
        "\"ant_aborto\",\n",
        "\"ant_obito\",\n",
        "\"ant_mnt\",\n",
        "\"ant_malformado\",\n",
        "\"ant_macrosomico\",\n",
        "\"cesarea_iter\",\n",
        "\"ema\",\n",
        "\"alta_paridad\",\n",
        "\"estatura_baja\",\n",
        "\"pgr\",\n",
        "\"infeccion_pregesta\",\n",
        "\"embarazo_multiple\",\n",
        "\"preeclampsia\",\n",
        "\"polihidramnios\",\n",
        "\"hidramnios\",\n",
        "\"infeccion_gesta\",\n",
        "\"ac_fol_pregesta\",\n",
        "\"ac_fol_gesta\",\n",
        "\"sdg_fol\",\n",
        "\"trim_fol\",\n",
        "\"ac_fol_dosis_mg\",\n",
        "\"modo_parto\",\n",
        "\"sdg_parto\",\n",
        "\"termino_rn\",\n",
        "\"macrosomia_rn\",\n",
        "\"malformaciones_rn\",\n",
        "\"aborto_rn\",\n",
        "\"obito_rn\",\n",
        "\"mnt_rn\",\n",
        "\"ins_dx\",\n",
        "\"gluc_dx\",\n",
        "\"crea_dx\",\n",
        "\"urico_dx\",\n",
        "\"tg_dx\",\n",
        "\"ct_dx\",\n",
        "\"hdl_dx\",\n",
        "\"ldl_dx\",\n",
        "\"apob_dx\",\n",
        "\"lep_dx\",\n",
        "\"adipo_dx\",\n",
        "\"pcr_dx\",\n",
        "\"origen_px\",\n",
        "\"tiene_pareja\",\n",
        "\"paciente_trabaja\",\n",
        "\"escolaridad\",\n",
        "\"puntos_ses\",\n",
        "\"nivel_ses\",\n",
        "\"trimestre_ev1\",\n",
        "\"AcFolico_supl\",\n",
        "#\"AcFolico_marca\", # No tiene valor en este estudio\n",
        "\"AcFsup_dosis_mg\",\n",
        "\"supl_preconcep\",\n",
        "\"supl_1erTrim\",\n",
        "\"Multi0_acfol1\",\n",
        "\"iniciosupl_sdg\",\n",
        "\"AcFolico_papa\",\n",
        "\"Ins_1av\",\n",
        "\"Gluc_1av\",\n",
        "\"Col_1av\",\n",
        "\"Tg_1av\",\n",
        "\"LDL_1av\",\n",
        "\"HDL_1av\",\n",
        "\"CMB_1av\",\n",
        "\"PCT_1av\",\n",
        "\"CMB_2av\",\n",
        "\"PCT_2av\"\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "excel_file_1 = \"data/base_multimorb_fase1_fase_2_clinicas_originales.xlsx\"\n",
        "\n",
        "df1_sheet1 = pd.read_excel(\n",
        "    excel_file_1,\n",
        "    sheet_name=0,\n",
        "    usecols=cols_to_include,\n",
        ")\n",
        "\n",
        "df1_sheet2 = pd.read_excel(\n",
        "    excel_file_1,\n",
        "    sheet_name=1,\n",
        "    usecols=cols_to_include,\n",
        ")\n",
        "\n",
        "df1_sheets = [\n",
        "    df1_sheet1,\n",
        "    df1_sheet2\n",
        "]\n",
        "\n",
        "df1 = pd.concat(df1_sheets)\n",
        "df1.to_csv(\"primera_Salida.csv\")\n",
        "#df1.describe()\n",
        "\n",
        "df1.describe(include=\"all\").T\n",
        "\n",
        "dfIds = df1[[\"id_gdg\", \"origen_px\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ-fJOqWXX8X"
      },
      "outputs": [],
      "source": [
        "df1.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA80zHd9XcGN"
      },
      "outputs": [],
      "source": [
        "df1.tail(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JBQIO1zXgRa"
      },
      "source": [
        "## ydata-profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PFDu4pxXq-F"
      },
      "source": [
        "### Generación de análisis univariado y bi/multivariado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xldFjkb0ZOu1"
      },
      "outputs": [],
      "source": [
        "if _BLN_REPORTS==True:\n",
        "  profile1 = ProfileReport(\n",
        "      df1,\n",
        "      title=\"Minimal Profiling Report for df1\",\n",
        "      # explorative=True,\n",
        "      minimal=True,\n",
        "  )\n",
        "  profile1.to_file(\"data/reports/profile1_min_report.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp4m1fvVZQlP"
      },
      "outputs": [],
      "source": [
        "if _BLN_REPORTS==True:\n",
        "  profile1.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LnBJW19ZZrW"
      },
      "source": [
        "### Debido al tamaño del reporte, se prefirió generar un archivo html llamado  profile1_exp_report.html que está alojado en el repositorio o puede ser consultado en la siguiente liga [EDA_Extensivo](/content/data/reports/profile1_min_report.html):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sLHZV3jZc7u"
      },
      "outputs": [],
      "source": [
        "if _BLN_REPORTS==True:\n",
        "  profile1_exp = ProfileReport(\n",
        "      df1,\n",
        "      title=\"Explorative Profiling Report for df1\",\n",
        "      explorative=True,\n",
        "      # minimal=True,\n",
        "  )\n",
        "  profile1_exp.to_file(\"data/reports/profile1_min_report.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUYOr_HEZhBH"
      },
      "source": [
        "### sweetviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp69_W7yZl9O"
      },
      "outputs": [],
      "source": [
        "# Missingness Map\n",
        "\n",
        "import missingno as msno\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(40, 40))  # Adjust size as needed\n",
        "\n",
        "# Create the heatmap\n",
        "msno.heatmap(df1, ax=ax)\n",
        "\n",
        "# Reduce font size of tick labels (optional)\n",
        "ax.tick_params(axis='both', which='major', labelsize=10)  # Adjust size as needed\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHVnw2JcZrx3"
      },
      "source": [
        "Esta gráfica nos permite identificar zonas de alta correlación con presencia y ausencia de variables.\n",
        "\n",
        "Por ejemplo, la zona de variables ac_fol_pregesta, ac_fol_gesta, sdg_fol, trim_fol y ac_fol_dosis_mg tiene una ligera correlación positiva en presencia de biopsias.\n",
        "\n",
        "Misma conducta podemos observarla con las variables trimestre_ev1, acFolico_supl, AcFolico_Marca, AcFsup_dosis_mg, supl_preconcep, supl_1erTrim, Multi0_acfol1, inicisupl_sdg, AcFolico_papa, Gluc_1av, Col_1av, Tg_1av, LDL_1av, HDL_1av, CMB_1av, PCT_1av.\n",
        "\n",
        "También encontramos correlaciones positivas en presencia de datos con las siguientes pares de variables:\n",
        "\n",
        "[trimestre_ev1, acFolico_supl, AcFolico_Marca, AcFsup_dosis_mg, supl_preconcep, supl_1erTrim, Multi0_acfol1, inicisupl_sdg, AcFolico_papa, Gluc_1av, Col_1av, Tg_1av, LDL_1av, HDL_1av, CMB_1av, PCT_1av]\n",
        "\n",
        "<center>vs</center>\n",
        "\n",
        "[modo_parto, sdg_parto, termino_rn (candidata a variable dependiente), macromosima_rn (candidata a variable dependiente), malformaciones_rn, aborto_rn, obito_rn, mnt_rn, ins_dx, crea_dx, urico_dx, aprob_dx, lep_dx, pcr_dx]\n",
        "\n",
        "Estos últimos siendo parte del conjunto de marcadores bioquímicos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7JBKUWtaTmb"
      },
      "source": [
        "### Dendrograma\n",
        "\n",
        "Este gráfico proporciona información en forma de árbol generado mediante agrupación jerárquica y agrupa columnas que tienen fuertes correlaciones en nulidad.\n",
        "\n",
        "Si varias columnas se agrupan en el nivel cero, esto significa que la presencia de nulos en una de esas columnas está directamente relacionada con la presencia o ausencia de nulos en las demás columnas. Cuanto más separadas estén las columnas en el árbol, menos probable será que los valores nulos estén correlacionados entre las columnas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1iWbSsgaZrn"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(40, 40))\n",
        "\n",
        "msno.dendrogram(df1, ax=ax)\n",
        "\n",
        "ax.tick_params(axis='both', which='major', labelsize=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBoN37e2agR3"
      },
      "source": [
        "Como se mencionó, este gráfico nos permite entender diferentes grupos de columnas que guardan relación con su presencia o no de datos. Por ejemplo, el grupo de preeclampsia, polihidramnios e hidramnios están fuertemente correlacionados en nulidad. La presencia de una variable en este grupo indica que existe una alta probabilidad de la existencia de otra.\n",
        "\n",
        "Esto es importante porque gracias a esta gráfica podemos determinar grupos de variables y determinar qué técnica de imputación de datos podemos realizar. Es decir, todo lo que esté relacionado con la glucosa, puede ser tratado con la misma técnica de imputación.\n",
        "\n",
        "Inclusive podemos identificar clusters de columnas a nivel cero como obs_h, talla, estatura_baja, ev_1ertrim, ev_dieta, edad_dx y ema que indican tener una gran densidad de dato."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiPoHpONal4L"
      },
      "source": [
        "### Densidad\n",
        "\n",
        "Además de los análisis anteriores, evaluamos el nivel de densidad de las características. Es decir:\n",
        "\n",
        "$$ Porosidad = TotalCeldasVacias / TotalFilas\n",
        "$$\n",
        "​\n",
        "\n",
        "Esto se calcula a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoR0Qxttaqm7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "# Adjust the max width of output cells to fit the screen\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
        "\n",
        "# Hide the vertical scroll bar for output cells\n",
        "display(HTML(\"<style>.output { overflow-y: hidden; }</style>\"))\n",
        "\n",
        "\n",
        "# Graficamos la porosidad inicial\n",
        "getPorosidad(df1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZBz7EqaaxJG"
      },
      "source": [
        "Las columnas en naranja son las variables que utilizaremos para el cálculo de la variable dependiente  `IndexMorbilidad`\n",
        "\n",
        "Gráfica de quintiles para determinar qué variables podemos recomendar eliminar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmuUpiSMbeQ4"
      },
      "outputs": [],
      "source": [
        "getPorosidadQuintiles(df1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKVGIQhsbjVY"
      },
      "source": [
        "Las siguientes variables que se encuentran en el último quintil, serán recomendada eliminarlas de la matriz de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMqBU2DPbmKq"
      },
      "outputs": [],
      "source": [
        "getVariablesUltimoQuintil(df1)\n",
        "\n",
        "variables_ultimo_quintil = ['adipo_dx', 'Multi0_acfol1', 'AcFsup_dosis_mg', 'trimestre_ev1', 'lep_dx', 'PCT_1av', 'CMB_1av', 'Gluc_1av', 'Tg_1av', 'Col_1av', 'HDL_1av',\n",
        " 'LDL_1av', 'crea_dx', 'CMB_2av', 'PCT_2av', 'urico_dx', 'apob_dx', 'pcr_dx', 'Ins_1av']\n",
        "\n",
        "\n",
        "df1.drop(columns =variables_ultimo_quintil, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRnMlyDebrSG"
      },
      "source": [
        "\n",
        "Podemos observar que la variable dependiente candidata `sdg_parto` tiene un 49.5% de densidad. Esto no permitirá tener un modelo fiable, ya que, si optamos por la eliminación de filas sin variable dependiente, solo tendríamos **1,188 * (1-0.495) = 599** (redondeado) observaciones.\n",
        "\n",
        "Por lo tanto, crearemos una variable dependiente (en la fase Ingeniería de Características) llamada `IndexMorbilidad` considerando los siguientes factores de morbilidad de la base de datos:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oTj013wcGY3"
      },
      "source": [
        "## Preprocesamiento\n",
        "\n",
        "Realizaremos las siguientes acciones:\n",
        "- Data imputation mediante el método multivariate feature imputation\n",
        "- Generación de variable dependiente mediante la creación de un índice de morbilidades.\n",
        "\n",
        "Además, eliminaremos las características que se encuentran en el último quintil de porosidad, esto previamente consultado con el equipo de investigación.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raRh7GFecLbN"
      },
      "source": [
        "## Data Imputation\n",
        "\n",
        "Utilizaremos el método Multivariate feature imputation que modela cada característica con valores perdidos en función de otras características y utiliza esa estimación para la imputación.\n",
        "\n",
        "Lo hace de forma iterativa: en cada paso, una columna de características se designa como salida y y las demás columnas de características se tratan como entradas X. Se ajusta un regresor en (X, y) para y conocido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEVFJHiUcGB4"
      },
      "outputs": [],
      "source": [
        "# Lo primero es sustituir los NaN's por celdas vacías para diferenciar limpiamente categóricas de numéricas.\n",
        "# Un valor NaN confundirá una variable numérica con un vector = [0, 1, 'NA', null] en categórica\n",
        "\n",
        "# Solo la variable macrosomia_rn es categórica\n",
        "\n",
        "df_wo_spaces = df1.fillna('')\n",
        "df_cleaned = df_wo_spaces.replace('NA', '')\n",
        "df_cleaned = df_cleaned.replace('Na', '')\n",
        "df_cleaned = df_cleaned.replace('na', '')\n",
        "\n",
        "# Encontramos typos en la base. Colocaron 'o' (vocal o) en lugar de 0\n",
        "# Será necesario reemplazar\n",
        "df_cleaned = df_cleaned.replace('o', 0)\n",
        "# Siguiente paso: identificar variables dicotómicas para ejecutar la correcta estrategia de imputación\n",
        "\n",
        "binary_features = identify_binary(df_cleaned.copy())  # Use a copy to avoid modifying original data\n",
        "all_cols = set(df_cleaned.columns)\n",
        "\n",
        "\n",
        "cols_to_remove_from_numeric =[\"macrosomia_rn\", \"id_gdg\", \"origen_px\"]\n",
        "\n",
        "numeric_features = [x for x in all_cols if x not in binary_features]\n",
        "numeric_features = [x for x in numeric_features if x not in cols_to_remove_from_numeric]\n",
        "\n",
        "categorical_features  =[\"macrosomia_rn\"]\n",
        "\n",
        "#df_cleaned[binary_columns] = df_cleaned[binary_columns].astype(bool)\n",
        "\n",
        "print(f\"Columnas dicotómicas: {binary_features}\")\n",
        "print(f\"Columnas numéricas: {numeric_features}\")\n",
        "print(f\"Columnas categorica: {categorical_features}\")\n",
        "\n",
        "df_cleaned['macrosomia_rn'] = df_cleaned['macrosomia_rn'].astype(str)\n",
        "\n",
        "df_cleaned.to_csv(\"salida.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MtUro4GcW7R"
      },
      "source": [
        "Eliminaremos las variables `id_gdg` y `origen_px` pues son identificadores. No es necesario imputarlos.\n",
        "\n",
        "Ahora, será necesario aplicar un algoritmo de imputación, en este caso hemos optado por Multivariate feature imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIh9-ekhce_u"
      },
      "source": [
        "## Data imputation para variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBMpfosrcl7N"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
        "\n",
        "df_cleaned.replace('', np.nan, inplace=True)\n",
        "\n",
        "\n",
        "# Create pipeline for preprocessing\n",
        "binary_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with mode\n",
        "    ('binarizer', FunctionTransformer(lambda x: x.astype(bool), validate=False))  # Convert to binary\n",
        "])\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n",
        "    ('scaler', StandardScaler())  # Scale numerical features if needed\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Encode categorical variables\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('binary', binary_transformer, binary_features),\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "pipeline.fit(df_cleaned)\n",
        "\n",
        "# Fit and transform the data\n",
        "transformed_data = pipeline.transform(df_cleaned)\n",
        "\n",
        "\n",
        "# Get feature names\n",
        "binary_feature_names = binary_features\n",
        "numeric_feature_names = numeric_features\n",
        "categorical_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
        "feature_names = binary_feature_names + numeric_feature_names + list(categorical_feature_names)\n",
        "\n",
        "transformed_df = pd.DataFrame(transformed_data, columns=feature_names)\n",
        "print(transformed_df)\n",
        "\n",
        "transformed_df.to_csv(\"final.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6gWMJz3cp7-"
      },
      "outputs": [],
      "source": [
        "# Es necesario ahora añadir el identificador y el estado origen de la República Mexicana de la madre\n",
        "dfIds.reset_index(drop=True, inplace=True)\n",
        "transformed_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "dfFinal = pd.concat([dfIds, transformed_df], axis=1)\n",
        "\n",
        "dfFinal.head()\n",
        "dfFinal.to_csv(\"dfV1.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GsNCNRBctMR"
      },
      "source": [
        "Hemos realizado la imputación de datos, comprobemos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS6MDcxJc0C4"
      },
      "outputs": [],
      "source": [
        "plot_distribution(transformed_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq03Glvmc7wk"
      },
      "source": [
        "En un vistazo a las gráficas de distribución se puede observar que ya tenemos una densidad valiosa y además, nuestra imputación en las variables binarias fue exitosa pues tenemos gráficas con valores de 1 o 0. Además, hemos escalado las variables numéricas para la preparación de la ingeniería de características."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4otDzagdAKQ"
      },
      "source": [
        "## Evaluación de porosidad\n",
        "\n",
        "En este código, revisamos cuántos valores nulos tenemos:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2tpxnxcdD-p"
      },
      "outputs": [],
      "source": [
        "null_counts_by_column = transformed_df.isnull().sum(axis=0)\n",
        "null_counts_by_column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwOahVb3dHvw"
      },
      "source": [
        "Ahora la gráfica para verificar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_9wJX_0dL5t"
      },
      "outputs": [],
      "source": [
        "getPorosidad(transformed_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV2bDWyceCO0"
      },
      "source": [
        "## Análisis bivariado\n",
        "\n",
        "A continuación, realizaremos el análisis bivariado para identificar relaciones interesantes como correlaciones entre variables. El objetivo será eliminar dichas variahbles o aplicar alguna transformación para generar un entrenamiento más rápido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMbOd21MeH2t"
      },
      "outputs": [],
      "source": [
        "# Para una mejor visualización de las correlaciones, usaremos un heatmap\n",
        "plt.figure(figsize=(40, 20))\n",
        "sns.heatmap(transformed_df[numeric_features].corr(), annot = True, vmin = -1, vmax = 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glhls9Vqfhe3"
      },
      "source": [
        "### Observaciones análisis bivariado.\n",
        "\n",
        "- Peso_1, Peso_2 y Peso_3 tienen una correlación positiva con el imc_h\n",
        "- Existe una correlación de ct_dx y tg_dx\n",
        "- Encontramos una correlación positiva fuerte con ct_dx e idl_dx\n",
        "- Peso_3 está fuertemente correlacionada con Peso_1\n",
        "- También Peso_2 tiene una correlación positiva con Peso_1\n",
        "- Gluc_dx_60 tiene una correlación positiva con auc\n",
        "- Gluc_dx_120 guarda una correlación positiva con auc\n",
        "- Gluc_d_x 0 con auc\n",
        "- Gluc_dx_180 con auc\n",
        "- Las variables relacionadas con glucosa_0_dx, 60_dx, 120_dx y 180_dx guarda correlaciones positivas\n",
        "- iniciosupl_sdg también tiene una correlación positiva con sdg_fol\n",
        "-idl_dx está correlacionada positivamente con ct_dx\n",
        "- nivel_ses con puntos_ses también están fuertemente correlacionada positivamente\n",
        "\n",
        "En general, un grupo de variables altamente correlacionadas no traerán información adicional al modelo e incrementarán la complejidad del algoritmo, esto aumentará el riesgo de comenter errores. Por tal motivo, se realizará una transformación del dataset con PCA para combatir la gran dimensionalidad del dataframe curado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8pIm8MOfneu"
      },
      "source": [
        "## Detección de Outliers\n",
        "\n",
        "A continuación, ejecutaremos código para detección de outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjkqozg_fszl"
      },
      "outputs": [],
      "source": [
        "dfOutliers = transformed_df[numeric_features]\n",
        "outliers_IQR = detect_outliers_iqr(dfOutliers)\n",
        "\n",
        "# Create boxplot for each numerical variable\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.boxplot(data=dfOutliers, orient='h')\n",
        "plt.title(\"Boxplot of Numerical Variables\")\n",
        "plt.xlabel(\"Values\")\n",
        "plt.ylabel(\"Variables\")\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBtWxTdvfyp1"
      },
      "source": [
        "Ahora sí es claro saber qué variables tienen outliers. Dependiendo del modelo a elegir posteriormente, será importante revisar si será necesario aplicar alguna transformación como box-cox para evitar problemas al momento de entrenar. Considerar que la metodología CRISP nos permite iterar entre fases con lo cual podemos experimentar con algoritmos que no son muy sensibles a los outliers como Random Forest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMvXKwQdf3Ex"
      },
      "source": [
        "## Selección de variable dependiente\n",
        "\n",
        "En conjunto con el equipo médico, se ha decidido que la variable dependiente será llamada  `IndexMorbilidaderá` compuesta por la suma de presencias en cada una de las siguientes variables:\n",
        "\n",
        "- malformaciones_rn\n",
        "- aborto_rn\n",
        "- obito_rn\n",
        "- mnt_rn\n",
        "- preeclampsia\n",
        "- polihidramnios\n",
        "- hidramnios\n",
        "- infeccion_gesta\n",
        "\n",
        "Es decir, `IndexMorbilidad` tendrá la siguiente escala:\n",
        "\n",
        "- **Clase A**. Si contiene 8 de ellas\n",
        "- **Clase B**. Si contiene entre 4 y 7 de ellas\n",
        "- **Clase C**. Si contiene entre 1 y 3 de ellas\n",
        "- **Clase D**. Si no contiene ninguna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfX9cFTaf71H"
      },
      "outputs": [],
      "source": [
        "proposed_y_columns = [\n",
        "    \"malformaciones_rn\",\n",
        "    \"aborto_rn\",\n",
        "    \"obito_rn\",\n",
        "    \"mnt_rn\",\n",
        "    \"preeclampsia\",\n",
        "    \"polihidramnios\",\n",
        "    \"hidramnios\",\n",
        "    \"infeccion_gesta\",\n",
        "]\n",
        "\n",
        "X = transformed_df.copy().drop(columns=proposed_y_columns)\n",
        "\n",
        "wip_y = transformed_df.copy()[proposed_y_columns]\n",
        "\n",
        "wip_y[\"sum\"] = wip_y.sum(axis=1)\n",
        "\n",
        "value_ranges = [0, 1, 4, 7, 8]\n",
        "bin_labels = [\"D\", \"C\", \"B\", \"A\"]\n",
        "\n",
        "wip_y[\"IndexMorbilidad\"] = pd.cut(wip_y[\"sum\"], bins=value_ranges, labels=bin_labels, right=False)\n",
        "\n",
        "# wip_y[wip_y[\"sum\"]>=1].describe()\n",
        "\n",
        "y = wip_y.copy()[\"IndexMorbilidad\"]\n",
        "\n",
        "y.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwZXvYP5gAlX"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swQCEHwEgEg1"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='-')\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
        "plt.title(\"Elbow Method for Choosing Number of Components\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6QukdPegI0m"
      },
      "outputs": [],
      "source": [
        "threshold = 0.99\n",
        "threshold_index = np.argmax(np.cumsum(pca.explained_variance_ratio_) > threshold)\n",
        "print(\"Number of Components required to maintain at least {:.2%} of variance: {}\".format(threshold, threshold_index+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVNuIvXlgMcB"
      },
      "outputs": [],
      "source": [
        "pca_threshold = PCA(n_components=threshold_index+1)\n",
        "pca_threshold.fit(X)\n",
        "X_pca = pca_threshold.transform(X)\n",
        "X_pca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En0DVOnHgPt7"
      },
      "source": [
        "## Clustering with K-means after applying PCA dimensionality reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeTV84NdgUq4"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "cluster_range = range(2, 6)\n",
        "\n",
        "fig, axs = plt.subplots(\n",
        "    len(cluster_range),\n",
        "    1,\n",
        "    figsize=(8, len(cluster_range) * 4),\n",
        "    # subplot_kw=dict(projection=\"3d\") # Uncomment for 3D projection\n",
        ")\n",
        "\n",
        "for i, n_clusters in enumerate(cluster_range):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=22)\n",
        "    cluster_labels = kmeans.fit_predict(X_pca)\n",
        "    silhouette_avg = silhouette_score(X_pca, cluster_labels)\n",
        "\n",
        "    axs[i].scatter(\n",
        "        X_pca[:, 0],\n",
        "        X_pca[:, 1],\n",
        "        # X_pca[:, 2], # Uncomment for 3D projection\n",
        "        c=cluster_labels,\n",
        "        cmap=\"viridis\",\n",
        "    )\n",
        "    axs[i].set_title(f\"K-means  with {n_clusters} Clusters & Silhouette Score: {silhouette_avg}\")\n",
        "    axs[i].set_xlabel(\"Principal Component 1\")\n",
        "    axs[i].set_ylabel(\"Principal Component 2\")\n",
        "    # axs[i].set_zlabel(\"Principal Component 3\") # Uncomment for 3D projection\n",
        "    axs[i].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzmwXjKoiwcr"
      },
      "source": [
        "# Ingeniería de características"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVzMBFVBitzC"
      },
      "source": [
        "## 1. Construcción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3SO0LCBipX_"
      },
      "source": [
        "\n",
        "Tomaremos el dataframe `dfPreparado` que preparamos en la **Fase I**, aplicaremos la construcción de la variable dependiente y la inclusión de nuevas características. Por ejemplo, la variable `prematurez` se basará en el número de semanas de la variable `sdg_parto`.\n",
        "\n",
        "A continuación las reglas de creación de características:\n",
        "\n",
        "- **Prematurez** [variable nueva] En función al valor de la variable `sdg_parto` se determinará si el valor es **1** o **0**. La regla es: Si el valor es menor a 36, entonces será prematuro (1), caso contrario el valor será no prematuro (0).\n",
        "- **EscalaRiesgo** [variable dependiente] Clasificación que tomará como base la suma de '1' en las 8 características indicadas por el cuerpo médico + nueva característica: `prematurez`. Con base en este valor, obtendremos los siguientes valores:\n",
        "  - **A**: Alto riesgo: Puntuación 3 y 4\n",
        "  - **B**: Medio riesgo: Puntuación entre 1 y 2\n",
        "  - **C**: Bajo Riesgo: Puntuación 0\n",
        "\n",
        "- **Clúster asociado** Después de correr un algoritmo de clustering, agregaremos al dataframe _dfPreparado_ el número de clúster asociado.\n",
        "\n",
        "Además, crearemos variables con one hot encoding para la variable `macrosomia_rn`. Dicha variable tiene un vector de valores que comprende ratios, por ejemplo: 1/0 y 0/0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-nysKnAi5xS"
      },
      "source": [
        "### 1.1. Funciones ad hoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVk3PEaVjods"
      },
      "outputs": [],
      "source": [
        "def definePrematurez(sdg_parto):\n",
        "  if sdg_parto > 36:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "def defineClase(valor_IndexMorbilidad):\n",
        "  if valor_IndexMorbilidad ==3 or valor_IndexMorbilidad==4:\n",
        "    return \"A\"\n",
        "  elif valor_IndexMorbilidad ==1 or valor_IndexMorbilidad==2:\n",
        "    return \"B\"\n",
        "  else:\n",
        "    return \"C\"\n",
        "\n",
        "def identify_binary(data):\n",
        "\n",
        "  binary_cols = []\n",
        "  for col in data.columns:\n",
        "    unique_values = data[col].dropna().unique()\n",
        "    #print(f'Col name {col} unique values {unique_values}')\n",
        "\n",
        "    if len(unique_values) == 2 and (set(unique_values) == {0, 1}):\n",
        "\n",
        "      binary_cols.append(col)\n",
        "  return binary_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vptkg-PFjp9E"
      },
      "source": [
        "### 1.2 Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yudOmF9jwcs"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "# Para el clustering\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT3woESkj1Bf"
      },
      "source": [
        "### 1.3 Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOKPP0ivj45d"
      },
      "outputs": [],
      "source": [
        "# Adquisisicón de datos\n",
        "\n",
        "dfPreparado = pd.read_csv(\"data/dataset_proxima_fase.csv\", encoding='utf8')\n",
        "dfIds = dfPreparado[['id_gdg', 'origen_px']]\n",
        "dfPreparado.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vOjTv7akB4K"
      },
      "source": [
        "### 1.4 Variable Prematurez"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZOAL3-okMYu"
      },
      "outputs": [],
      "source": [
        "dfPreparado[\"prematurez\"] = dfPreparado['sdg_parto'].apply(lambda x:definePrematurez(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrB7mz95kRB5"
      },
      "source": [
        "#### 1.4.1 Revisar distribución de la nueva variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrXGPxujkULH"
      },
      "outputs": [],
      "source": [
        "prematurez_counts = dfPreparado['prematurez'].value_counts()\n",
        "prematurez_counts = prematurez_counts.sort_index()\n",
        "plt.bar(prematurez_counts.index, prematurez_counts.values, color='skyblue')\n",
        "plt.xticks(list(map(int, prematurez_counts.index)))\n",
        "\n",
        "plt.title('Prematurez Bar Chart')\n",
        "plt.xlabel('Prematurez')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_viS_SjkYpU"
      },
      "source": [
        "### 1.5 Variable dependiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibQqM5iCkbPn"
      },
      "outputs": [],
      "source": [
        "lst_IndexMorbilidad = ['malformaciones_rn', 'aborto_rn','obito_rn','mnt_rn','preeclampsia', 'polihidramnios', 'hidramnios', 'infeccion_gesta', 'prematurez']\n",
        "dfPreparado[\"IndexMorbilidad\"] = dfPreparado[lst_IndexMorbilidad].sum(axis=1)\n",
        "dfPreparado[\"EscalaRiesgo\"] =  dfPreparado['IndexMorbilidad'].apply(lambda x:defineClase(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKbDbHX2kh0i"
      },
      "source": [
        "#### 1.6 Revisar la distribución de la variable dependiente **EscalaRiesgo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ERDjQ8Skgwv"
      },
      "outputs": [],
      "source": [
        "escala_riesgo = dfPreparado['EscalaRiesgo'].value_counts()\n",
        "escala_riesgo = escala_riesgo.sort_index()\n",
        "plt.bar(escala_riesgo.index, escala_riesgo.values, color='blue')\n",
        "plt.title('Escala Riesgo Bar Chart')\n",
        "plt.xlabel('Escala Riesgo')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5aggMU5koIz"
      },
      "outputs": [],
      "source": [
        "dfPreparado.to_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaZCyRvEksb-"
      },
      "source": [
        "## 2. Normalización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5tyHqL1kwKw"
      },
      "source": [
        "En esta fase, realizaremos escalamiento de las variables numéricas pues, derivado del análisis de la **Fase 1** encontramos diferentes outliers que pueden desestabilizar el modelo base y subsecuentes.\n",
        "\n",
        "Utilizaremos un Pipeline para crear la canalización para aplicar las transformaciones pertinentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVA7F0N8kxtG"
      },
      "outputs": [],
      "source": [
        "binary_features = identify_binary(dfPreparado)\n",
        "\n",
        "all_cols = set(dfPreparado.columns)\n",
        "cols_to_remove_from_numeric =[\"macrosomia_rn\", \"id_gdg\", \"origen_px\"]\n",
        "\n",
        "numeric_features = [x for x in all_cols if x not in binary_features]\n",
        "numeric_features = [x for x in numeric_features if x not in cols_to_remove_from_numeric]\n",
        "numeric_features.remove(\"IndexMorbilidad\")\n",
        "numeric_features.remove(\"EscalaRiesgo\")\n",
        "numeric_features.remove(\"anticonceptivo\")\n",
        "\n",
        "categorical_features  =[\"macrosomia_rn\", \"anticonceptivo\"]\n",
        "\n",
        "print(binary_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1EvFAQck4Pp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create pipeline for preprocessing\n",
        "binary_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with mode\n",
        "    ('binarizer', FunctionTransformer(lambda x: x.astype(bool), validate=False))  # Convert to binary\n",
        "])\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())  # Scale numerical features if needed\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Encode categorical variables\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('binary', binary_transformer, binary_features),\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "pipeline.fit(dfPreparado)\n",
        "\n",
        "# Fit and transform the data\n",
        "transformed_data = pipeline.transform(dfPreparado)\n",
        "\n",
        "\n",
        "# Get feature names\n",
        "binary_feature_names = binary_features\n",
        "numeric_feature_names = numeric_features\n",
        "categorical_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
        "feature_names = binary_feature_names + numeric_feature_names + list(categorical_feature_names)\n",
        "\n",
        "transformed_df = pd.DataFrame(transformed_data, columns=feature_names)\n",
        "transformed_df.to_csv(\"final.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh1rN4Dgk8tt"
      },
      "source": [
        "## 3. Selección / Extracción\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJAePTE-k9jj"
      },
      "source": [
        "En esta fase incluiremos la evaluación por clustering y agregaremos como característica de identificación el cluster identificado.\n",
        "\n",
        "Esto se realiza hasta esta fase debido a que se procesaron, imputaron y escalaron primero los datos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxa16I18mH3Q"
      },
      "source": [
        "### 3.1 Evaluación del número óptimo de clústers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXlakh4HmMKS"
      },
      "source": [
        "De acuerdo a esta primera evaluación, es posible apreciar que **3** es el número adecuado de clusters para el dataset. Podemos ver cómo la curva \"rompe\" en el cluster 3. Confirmaremos nuestra hipótesis con la gráfica de silueta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKR36q0ymPk0"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca.fit(transformed_df)\n",
        "df_pca = pca.transform(transformed_df)\n",
        "\n",
        "\n",
        "distortions = []\n",
        "for i in range(1, 11):  # Test cluster numbers from 1 to 10\n",
        "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
        "    kmeans.fit(df_pca)\n",
        "    distortions.append(kmeans.inertia_)\n",
        "\n",
        "\n",
        "plt.plot(range(1, 11), distortions, marker='o')\n",
        "plt.title('Elbow Chart')\n",
        "plt.xlabel('Número de clusters')\n",
        "plt.ylabel('Distortion')\n",
        "plt.xticks(range(1, 11))\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjArOqiUmUle"
      },
      "source": [
        "Ahora la gráfica de clustering:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaOfL-8FmgHc"
      },
      "outputs": [],
      "source": [
        "optimal_num_clusters = 3\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=optimal_num_clusters, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(df_pca)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df_pca[:, 0], df_pca[:, 1], c=cluster_labels, cmap='viridis', alpha=0.5)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('Clusters based on PCA')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvOaNHAumlbX"
      },
      "source": [
        "En este código evaluamos el score de silueta para entender qué tan bien están clasificados los puntos de acuerdo al número de clúster.\n",
        "\n",
        "- **Valores cercanos a 1:** Puntos bien clasificados y cercanos al centroide de grupo.\n",
        "- **Valores cercanos a 0:** Puntos entre el límite de 2 clusters\n",
        "- **Valores cercanos a -1:** Puntos mal clasificados y alejados del centroide de grupo.\n",
        "\n",
        "En este sentido, 2 clústers nos dan la mejor pertenencia. Sin embargo, agrupar por 3 tiene es más armónico con la variable dependiente `EscalaRiesgo`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8el5cR-amqK7"
      },
      "outputs": [],
      "source": [
        "range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "silhouette_scores = []\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(df_pca)\n",
        "\n",
        "    silhouette_avg = silhouette_score(df_pca, cluster_labels)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "    sample_silhouette_values = silhouette_samples(df_pca, cluster_labels)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlim([-0.1, 1])\n",
        "    ax.set_ylim([0, len(df_pca) + (n_clusters + 1) * 10])\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "        color = plt.cm.tab10(float(i) / n_clusters)\n",
        "        ax.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax.set_title(\"Silhouette plot for {} clusters\".format(n_clusters))\n",
        "    ax.set_xlabel(\"Silhouette coefficient values\")\n",
        "    ax.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "    ax.annotate('Average silhouette\\nscore: {:.2f}'.format(silhouette_avg), xy=(silhouette_avg, len(df_pca) * 0.95),\n",
        "                xytext=(silhouette_avg + 0.05, len(df_pca) * 0.95),\n",
        "                arrowprops=dict(facecolor='black', shrink=0.05))\n",
        "\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plt.plot(range_n_clusters, silhouette_scores, marker='o')\n",
        "plt.title('Silhouette Scores for Different Numbers of Clusters')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.xticks(range_n_clusters)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB3j6U71mutc"
      },
      "source": [
        "### 3.2 Asignación del número de cluster a cada observacion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_Zkt7PRmwBr"
      },
      "source": [
        "En esta parte del código, se asignará el número de cluster calculado a cada observación. Hemos definido **3** como el número ideal después de los análisis realizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fuRn9NOm3PE"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(transformed_df)\n",
        "\n",
        "\n",
        "transformed_df['cluster'] = cluster_labels\n",
        "\n",
        "# Agregar el id_gdg y origen_px para fines de visualización posterior\n",
        "\n",
        "dfFinal = pd.concat([dfIds, transformed_df], axis=1)\n",
        "dfFinal.to_csv(\"clusters.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6PPNc0Fm7ah"
      },
      "source": [
        "### 3.3 Identificación de componentes principales **(PCA)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUFx35Lzm8Rn"
      },
      "source": [
        "Con este análisis podremos identificar cuáles características suman información al modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EznibBienJZl"
      },
      "outputs": [],
      "source": [
        "pcs = PCA(n_components=85)\n",
        "\n",
        "principalComponents = pcs.fit_transform(transformed_df)\n",
        "scores_pca = pcs.transform(transformed_df)\n",
        "\n",
        "pcsSummary = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n",
        "                          'Proportion of variance':pcs.explained_variance_ratio_,\n",
        "                          'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)#column\n",
        "                          }\n",
        "                          )\n",
        "pcsSummary.round(4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2EfSAutnMxV"
      },
      "outputs": [],
      "source": [
        "\n",
        "loadings_df = pd.DataFrame(pcs.components_.T, columns=transformed_df.columns)\n",
        "loadings_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNBV2YVLnRjY"
      },
      "source": [
        "Con esta tabla podemos calcular fácilmente que:\n",
        "\n",
        "- Hasta el componente **18** podemos acumular 81% del total de varianza\n",
        "\n",
        "- Los componentes 0 y 1 son los que mayor valor aportan al dataset con 0.15 y 0.23 respectivamente.\n",
        "\n",
        "Grafiquemos estos resultados para tener una mejor claridad de las cosas.\n",
        "\n",
        "Como habíamos notado en el componente principal **18** acumulamos el **81%** sin embargo, por tratarse de un tema médico, aumentaremos nuestro umbral a **99%**. Encontramos que en el componente **51** alcanzamos este umbral."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnB9H5UynWSA"
      },
      "source": [
        "### 3.4 **Insight**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOntiBSsnXY3"
      },
      "source": [
        "Da mucho valor esta gráfica, pues partimos de una matriz transformada de 85 (menos 2 de identificación y estado de origen) columnas, ahora podemos quedarnos con solo 31 de estas características y facilitar el entrenamiento.\n",
        "\n",
        "Como habíamos mencionado, hasta el componente **51** tenemos alrededor del **99%** de la varianza. Consideramos suficiente esta concentración de información para nuestros análisis.\n",
        "\n",
        "Para completar el análisis de PCA, examinemos cada una de las variables para entender cuánto aportan de información a cada componente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At9iFi2ynmWv"
      },
      "outputs": [],
      "source": [
        "# Hagamos una gráfica para representarlo y determinar el número mínimo de componentes:\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "PC_components = np.arange(pcs.n_components_) + 1\n",
        "#PC_components\n",
        "\n",
        "_ = sns.set(style = 'whitegrid',\n",
        "            font_scale = .8, rc={'xtick.labelsize': 10}\n",
        "            )\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 7))\n",
        "\n",
        "_ = sns.barplot(x = PC_components,\n",
        "                y = pcs.explained_variance_ratio_,\n",
        "                color = 'b'\n",
        "                )\n",
        "\n",
        "_ = sns.lineplot(x = PC_components-1,\n",
        "                 y = np.cumsum(pcs.explained_variance_ratio_),\n",
        "                 color = 'black',\n",
        "                 linestyle = '-',\n",
        "                 linewidth = 2,\n",
        "                 marker = 'o',\n",
        "                 markersize = 4\n",
        "                 )\n",
        "plt.axvline(x=18, color='r', linestyle='--', linewidth=2)\n",
        "plt.text(18, 0.81, \"81% varianza\", color='r', fontsize=12, verticalalignment='bottom')\n",
        "# Add a vertical line at x=30 (for example)\n",
        "plt.axvline(x=51, color='g', linestyle='--', linewidth=2)\n",
        "\n",
        "# Add text \"99% varianza\" at x=30\n",
        "plt.text(51, 0.99, \"99% varianza\", color='g', fontsize=12, verticalalignment='bottom')\n",
        "\n",
        "plt.title('Variance Plot')\n",
        "plt.xlabel('N-th Principal Component')\n",
        "plt.ylabel('Variance Explained')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr-BQHtRntFY"
      },
      "source": [
        "## Conclusiones apartado\n",
        "\n",
        "Una vez realizada la primera fase del proyecto, consistente en el entendimiento del negocio, pasamos a la etapa de limpieza, escalado e imputación de los datos. En esta fase nos quedamos con 76 variables de los 99 originales, se creó una nueva variable llamada prematurez, la cual se agrega al nuevo conjunto de variables.\n",
        "\n",
        "Al no tener una variable dependiente única, decidimos crear un índice de morbilidad, este índice lo que busca es ser un indicador de riesgo, y toma como base las variables “malformaciones_rn”, “aborto_rn”,”obito_rn”,”mnt_rn”,”preeclampsia”, “polihidramnios”, “hidramnios”, “infección_gesta”, “prematurez”. Como se observa, se incluye la nueva variable creada “prematurez”.\n",
        "\n",
        "Al índice se le agregó una escala de riesgo denotada con “A” para Alto Riesgo, “B” para Riesgo Medio y “C” para Bajo Riesgo. Graficamos la escala de riesgo y nos muestra una baja proporción de alto riesgo, una proporción media de “Medio Riesgo” y una proporción grande de “Bajo Riesgo”.\n",
        "\n",
        "Derivado de outliers se realizó un proceso de escalamiento a fin de normalizar las variables y con ello poder tener mejores resultados.\n",
        "\n",
        "Aplicamos una reducción de dimensionalidad a fin de tener mejores datos para el entrenamiento de un modelo de clasificación pues a mayor número de características, mayor complejidad computacional, es decir, se requerirá recursos informáticos y por lo tanto mayor tiempo de entrenamiento. El resultado de esto fue que 3 sería el mejor número de clúster para aplicar, lo cual se comprobó con la curva Elbow.\n",
        "\n",
        "Con la técnica Principal Component Analysis identificamos cuantos componentes necesitamos para lograr el mejor resultado, encontramos que para lograr el 99% se requiere hasta el 51 componente y para determinar el mínimo serían 18 alcanzando un 81%.\n",
        "\n",
        "Dado que la metodología CRIPS es iterativa, podremos revisitar esta fase para experimentar con otros algoritmos de reducción de características.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiM7Xn5EoLe1"
      },
      "source": [
        "# IV Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ESqpG8Ngj79"
      },
      "source": [
        "\n",
        "\n",
        "Proyecto:\n",
        "\n",
        "*Modelo clasificador de multimorbilidad maternal y predictor de desenlaces perinatales a partir de datos clínicos metabólicos, genéticos y nutricionales de mujeres mexicanas*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3f055a7grDf"
      },
      "source": [
        "## 1. Algoritmo\n",
        "\n",
        "El problema escogido será tratado como aprendizaje supervisado. Nos enfocaremos en la predicción de una variable dependiente \"EscalaRiesgo\" que contendrá 3 valores:\n",
        "- A. Para alto riesgo (la presencia de 3 o 4 factores multimorbilidad)\n",
        "- B. Para medio riesgo (presencia de 1 o 2 factores multimorbilidad)\n",
        "- C. Para riesgo nulo. (sin la presencia de factores de multimorbilidad)\n",
        "\n",
        "En esta entrega utilizaremos un algoritmo de **Random Forest** por las siguientes causas:\n",
        "- **Precisión**. Los árboles de bosque aleatorios proporcionan buenos resultados cuando tenemos datos con muchas características.\n",
        "- **Manejo del sobreajuste.** Mediante el promedio de los resultados de los árboles, los bosques aleatorios reducen el riesgo de sobreajuste en comparación con un único árbol de decisión.\n",
        "- **Heterogeneidad.** Funcionan muy buen con datos tanto numéricos como categóricos\n",
        "- Son mucho menos sensibles a datos ruidosos.\n",
        "- **Generalización.** Dan muy buenos resultados en térrminos de generalización para nuevas observaciones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9ykt-Qdg22O"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, learning_curve\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xVLpXY6g8Mm"
      },
      "source": [
        "##1.2 Carga de datos limpios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y-gVP-yhBNe"
      },
      "outputs": [],
      "source": [
        "file_path = r'data/data_limpia.csv'\n",
        "\n",
        "#Eliminamos columnas que no necesitaremos como identificarores y el número de clúster\n",
        "\n",
        "cols_to_remove = ['id_gdg', 'origen_px', 'IndexMorbilidad', 'anticonceptivo_0.0',\n",
        "       'anticonceptivo_0.6316526610644257', 'anticonceptivo_1.0',\n",
        "       'anticonceptivo_2.0', 'cluster']\n",
        "\n",
        "data = pd.read_csv(file_path, sep=\";\", encoding='utf-8')\n",
        "data  = data.drop(cols_to_remove , axis=1)\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz1yLLIyhG5T"
      },
      "source": [
        "###1.2.1 Inspección rápida de la data cargada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y64KY2ExhKik"
      },
      "outputs": [],
      "source": [
        "data.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-A13q_ckXkR"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPvJIKrQkb6B"
      },
      "source": [
        "## 1.3 Generación de holdout set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M72lDbFDkfgf"
      },
      "outputs": [],
      "source": [
        "X = data.drop('EscalaRiesgo', axis=1)\n",
        "y = data['EscalaRiesgo']\n",
        "\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAvPM2YLkoLr"
      },
      "source": [
        "## 1.4 Entrenamiento del algoritmo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZxkULAykyiO"
      },
      "outputs": [],
      "source": [
        "# 3. Evaluate the main features towards EscalaRiesgo using RandomForestClassifier\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU01pCEvgw7J"
      },
      "source": [
        "## 2. Características Importantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSBl1QVMk6iW"
      },
      "outputs": [],
      "source": [
        "feature_importances = pd.DataFrame(model.feature_importances_, index=X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 12))\n",
        "sns.barplot(x=feature_importances.importance, y=feature_importances.index)\n",
        "plt.title('Feature Importance')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZgn0dvCk-VI"
      },
      "source": [
        "##2.1 Evaluación con observaciones no vistas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r2GYVjClBcC"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_holdout)\n",
        "print(classification_report(y_holdout, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvrpXC3jlFSS"
      },
      "source": [
        "A primera vista, nuestro modelo puede estar sobreajustado. No tenemos valores superiores a 0 en precisión y recall por lo tanto el F1-Score cae a valores de **0.66**\n",
        "\n",
        "No tener valores en precisión y recall además de una F1-Score en .66 son nuestras métricas base para afinar el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT7muNM7lJAO"
      },
      "source": [
        "##3. Sub / Sobreajuste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBUltVWclNqi"
      },
      "outputs": [],
      "source": [
        "train_scores = []\n",
        "val_scores = []\n",
        "\n",
        "cv = 5\n",
        "for train_idx, val_idx in StratifiedKFold(n_splits=cv).split(X_train, y_train):\n",
        "    X_cv_train, X_cv_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "    y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "    model.fit(X_cv_train, y_cv_train)\n",
        "\n",
        "    train_score = model.score(X_cv_train, y_cv_train)\n",
        "    val_score = model.score(X_cv_val, y_cv_val)\n",
        "\n",
        "    train_scores.append(train_score)\n",
        "    val_scores.append(val_score)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, cv+1), train_scores, label='Training Score')\n",
        "plt.plot(range(1, cv+1), val_scores, label='Validation Score')\n",
        "plt.title('Training vs Validation Scores')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05rqg-mIlSMW"
      },
      "source": [
        "Tenemos un modelo sobreajustado, el modelo ha memorizado las relaciones entrevariables. Esto es claro pues tenemos valores de **1.00** en el entrenamiento y valores de 0.98 en el set de validación. No tenemos generalización."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuC3qjRHlVnW"
      },
      "source": [
        "##4. Métrica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJlr57W_lbFv"
      },
      "source": [
        "Para este problema de clasificación multiclase, utilizamos classification_report que incluye:\n",
        "- Precisión: Indica la proporción de identificaciones positivas que fueron realmente correctas.\n",
        "- Recall: Indica la proporción de positivos reales que fueron identificados correctamente.\n",
        "- Puntuación F1: Media armónica de la precisión y la recuperación, que proporciona una medida única del rendimiento del modelo.\n",
        "- Soporte: Número de casos reales de la clase en el conjunto de datos especificado.\n",
        "\n",
        "La elección de estas métricas nos permite equilibrar la precisión y el recall, proporcionando una evaluación completa del rendimiento del modelo en todas las clases.\n",
        "\n",
        "Como se mencionó, nuestra **F1-score está en 0.66**\n",
        "\n",
        "La matriz de confusión soporta lo mal que performa el modelo con la clase **A** (alto riesgo)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u81ik41VlfzE"
      },
      "source": [
        "## 4.1 Matriz de confusion\n",
        "\n",
        "Una herramienta vital para el entendimiento del desemeño de nuestro modelo es la matriz de confusión, a continuación se codifica esta funcionalidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8N-9zKhlRuV"
      },
      "outputs": [],
      "source": [
        "print(confusion_matrix(y_holdout, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-hz0ip_ll3B"
      },
      "outputs": [],
      "source": [
        "conf_matrix = confusion_matrix(y_holdout, y_pred)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hhBOJ-RlqH9"
      },
      "source": [
        "Tenemos un desbalanceo de clases notable, la clase A no es predicha en ningún momento. Deberemos tratar esto, en la siguiente fase, con un algoritmo **SMOTE** (Synthetic Minority Over-sampling Technique) y como si fuera un caso de uso de fraude donde el fraude es tratado como una anomalía estadística. Aquí será similar, nos interesa un balanceo uniformne entre clases y no lo tenemos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjsIi2tRlvWu"
      },
      "source": [
        "Un punto interesante es que ejecutamos un algoritmo SVC y obtuvimos mejores resultados en la matriz de confusión; dicho algoritmo logró predecir correctamente 2 observaciones en la clase A.\n",
        "\n",
        "Definitivamente incluiremos SVC para la siguiente entrega y lo pondremos a competir vs RandomForest y una Red Neuronal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7rDMcjtl0RV"
      },
      "outputs": [],
      "source": [
        "feature_importances.to_csv('feature_importances.csv', index=True)\n",
        "pd.DataFrame(classification_report(y_holdout, y_pred, output_dict=True)).to_csv('classification_report.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKQa9ULll6e0"
      },
      "source": [
        "##5. Desempeño"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKS8YI-sl-qA"
      },
      "outputs": [],
      "source": [
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    RandomForestClassifier(random_state=42), X, y, cv=cv, n_jobs=-1,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10), scoring='accuracy')\n",
        "\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "val_scores_mean = np.mean(val_scores, axis=1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_sizes, train_scores_mean, label='Training Score')\n",
        "plt.plot(train_sizes, val_scores_mean, label='Validation Score')\n",
        "plt.title('Learning Curve')\n",
        "plt.xlabel('Training Size')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9iYtqZnmC1k"
      },
      "source": [
        "Observamos un desempeño sobreajustado. Esto debido a la alta dimensionalidad, el desbalanceo de clases y la falta de hiperparámetros. Como modelo base es aceptable comenzar con un modelo con estas características. Lo importante será afinar, balancear y aplicar diferentes modelos de clasificación y elegir el que devuelva mejores F1-Score y una ROC aceptable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkmSnkTioGTG"
      },
      "source": [
        "## Conclusiones apartado\n",
        "\n",
        "Despues de realizar el tratamiento de datos, en esta etapa creamos nuestro primer modelo, donde encontramos que estamos ante un conjunto de datos muy complejo.\n",
        "\n",
        "Em primer lugar identificamos las variables infeccion_gesta, prematurez,preeclampsia, sdg_parto, termino_rn, hidramnios, malformaciones_rn, auc, escolaridad, gluc_dx_60, ac_fol_gesta, gluc_dx_120, imc_h, infeccion_pregesta, gluc_dx_180, gluc_dx_0, polihidramnios, edad_dx, peso_2, modo_parto, ct_dx,tg_dx, proporcionan un poder acumulado predictivo de __0.80__ puntos, lo que nos indica que estas definitivamente estarán en el modelo, para el resto de las variables tendríamos que analizarlas nuevamente a fin de determinar las que no tienen un impacto signiticativo.\n",
        "\n",
        "En segundo lugar, entrenamos tres modelos: Random Forest Classifier, Support Vector Machines y CNN, nos quedamos con Random Forest Classifier, sin embargo analizaremos a profundidad los resultados especificos de los demás, en este caso, tenemos un modelo sobrenetrenado, esto a todas luces se puede detectar cuando tenemos métricas de 1.00 en el training score vs valores .95 en el validación score. Claramente nuestro modelo ha aprendido de los datos de entrenamiento y lo ha memorizado. No queremos esto, queremos generalización, es decir, la adaptación correcta de nuestro modelo a datos no vistos anteriormente extraídos del holdout set.\n",
        "\n",
        "Así que en las próximas fases, deberemos realizar afinamiento de hiperparámetros y reducir la dimensionalidad de nuestra dataset incluyendo el análisis de PCA que realizamos entregamos pasadas.\n",
        "\n",
        "En tercer lugar observamos que tenemos un desbalance en la variable dependiente EscalaRiesgo para los valores A lo cual obligará realizar un balanceo de clases en la próxima entrega."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuoqUVVgUnxA"
      },
      "source": [
        "# Modelos Alternativos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aod6AM6QM7it"
      },
      "source": [
        "\n",
        "\n",
        "### Table of Contents\n",
        "1. [Preparación](##Preparación)\n",
        "2. [Comparativa](##Comparativa)\n",
        "3. [Ajuste Fino](##AjusteFino)\n",
        "\n",
        "Proyecto:\n",
        "\n",
        "*Modelo clasificador de multimorbilidad maternal y predictor de desenlaces perinatales a partir de datos clínicos metabólicos, genéticos y nutricionales de mujeres mexicanas*\n",
        "\n",
        "23 de mayo de 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMdtbPcvLN5E"
      },
      "source": [
        "##1. [Preparación](##Preparación)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBUwvY5DwR3q"
      },
      "source": [
        "### **1.1 Carga de Librerías**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkzojFEeECaY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDzqIuHxwTwd"
      },
      "outputs": [],
      "source": [
        "#%pip install imbalanced-learn\n",
        "#%pip install scikit-learn tensorflow\n",
        "#%pip install scikeras\n",
        "#%pip install --upgrade scikit-learn\n",
        "#%pip uninstall scikit-learn imbalanced-learn\n",
        "#%pip install scikit-learn==1.0.2 imbalanced-learn==0.8.1\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, precision_recall_curve, auc, recall_score, precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier, AdaBoostClassifier, BaggingClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8Lu4uwdE5SA"
      },
      "source": [
        "### **1.2. Carga de datos**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "rJ75tgy5G_8N",
        "outputId": "8fa81ae2-45a3-4874-ab82-f415ef26f6b3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EscalaRiesgo</th>\n",
              "      <th>biopsias</th>\n",
              "      <th>obs_h</th>\n",
              "      <th>ichos_pregesta</th>\n",
              "      <th>hta_pregesta</th>\n",
              "      <th>sop</th>\n",
              "      <th>hipotiroidismo</th>\n",
              "      <th>hipertiroidismo</th>\n",
              "      <th>consumo_alcohol</th>\n",
              "      <th>consumo_tabaco</th>\n",
              "      <th>...</th>\n",
              "      <th>sdg_parto</th>\n",
              "      <th>ant_aborto</th>\n",
              "      <th>macrosomia_rn_0</th>\n",
              "      <th>macrosomia_rn_0.0</th>\n",
              "      <th>macrosomia_rn_0/0</th>\n",
              "      <th>macrosomia_rn_1</th>\n",
              "      <th>macrosomia_rn_1.0</th>\n",
              "      <th>macrosomia_rn_1/0</th>\n",
              "      <th>macrosomia_rn_1/1</th>\n",
              "      <th>macrosomia_rn_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.747297</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.747297</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.747297</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.747297</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.420203</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 82 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  EscalaRiesgo  biopsias  obs_h  ichos_pregesta  hta_pregesta  sop  \\\n",
              "0            C         0      0               0             0    0   \n",
              "1            C         0      0               0             1    0   \n",
              "2            C         0      1               0             1    0   \n",
              "3            B         0      0               0             1    0   \n",
              "4            C         0      1               0             1    0   \n",
              "\n",
              "   hipotiroidismo  hipertiroidismo  consumo_alcohol  consumo_tabaco  ...  \\\n",
              "0               0                0                0               0  ...   \n",
              "1               0                0                0               0  ...   \n",
              "2               0                0                0               0  ...   \n",
              "3               0                0                0               0  ...   \n",
              "4               0                0                0               0  ...   \n",
              "\n",
              "   sdg_parto  ant_aborto  macrosomia_rn_0  macrosomia_rn_0.0  \\\n",
              "0        0.0   -0.747297                0                  0   \n",
              "1        0.0   -0.747297                0                  0   \n",
              "2        0.0   -0.747297                0                  0   \n",
              "3        0.0   -0.747297                0                  0   \n",
              "4        0.0    1.420203                0                  0   \n",
              "\n",
              "   macrosomia_rn_0/0  macrosomia_rn_1  macrosomia_rn_1.0  macrosomia_rn_1/0  \\\n",
              "0                  0                1                  0                  0   \n",
              "1                  0                1                  0                  0   \n",
              "2                  0                1                  0                  0   \n",
              "3                  0                1                  0                  0   \n",
              "4                  0                1                  0                  0   \n",
              "\n",
              "   macrosomia_rn_1/1  macrosomia_rn_2  \n",
              "0                  0                0  \n",
              "1                  0                0  \n",
              "2                  0                0  \n",
              "3                  0                0  \n",
              "4                  0                0  \n",
              "\n",
              "[5 rows x 82 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_path = r'data/data_limpia.csv'\n",
        "\n",
        "#Eliminamos columnas que no necesitaremos como identificarores y el número de clúster\n",
        "\n",
        "cols_to_remove = ['id_gdg', 'origen_px', 'IndexMorbilidad', 'anticonceptivo_0.0',\n",
        "       'anticonceptivo_0.6316526610644257', 'anticonceptivo_1.0',\n",
        "       'anticonceptivo_2.0', 'cluster']\n",
        "\n",
        "data = pd.read_csv(file_path, sep=\";\", encoding='utf-8', index_col=False)\n",
        "data  = data.drop(cols_to_remove , axis=1)\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37GVFqx0E_dp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data.drop('EscalaRiesgo', axis=1)\n",
        "y = data['EscalaRiesgo']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5XImi-VFL0_"
      },
      "source": [
        "### **1.3 Aplicar balanceo de clases via SMOTE**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNm_VXrLI6De"
      },
      "source": [
        "Esta técnica es recomendada cuando tenemos clases desbalanceadas. Nuestro problema tiene 3 clases:\n",
        "\n",
        "- A Para alto riesgo (la presencia de 3 o 4 factores multimorbilidad)\n",
        "- B Para medio riesgo (presencia de 1 o 2 factores multimorbilidad)\n",
        "- C Para riesgo nulo. (sin la presencia de factores de multimorbilidad)\n",
        "\n",
        "En temas de salud es importante tener clases balanceadas, esto nos permitirá tener los siguientes beneficios:\n",
        "\n",
        "- Mejora en la precisión del modelo; en este sentido, detectar una clase **A** con alto riesgo de morbilidad es crucial para que el modelo entregue valor al área médica.\n",
        "\n",
        "- Equidad en la atención médico. Debemos aseguramos que nuestro modelo sea justo y equitativo sobre todo en temas de salud. Un desbalance puede provocar sesgos y lecturas erróneas.\n",
        "\n",
        "- Generalización. Un modelo con clases balanceadas generalizará mejor cuando trate nuevos datos. En el dominio de conocimiento de salud humana, esto es relevante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMVI1KSnwULF"
      },
      "source": [
        "## 2. [Comparativa](##Comparativa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZAk_tQxT_MO"
      },
      "source": [
        "### Evaluación con TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmI0G1Qj0_6O"
      },
      "source": [
        "Es necesario aplicar un LabelEncoder para la variable dependiente ya que es categórica y de tipo string en este momento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDbFxYu30_6P",
        "outputId": "edfdb41f-931d-422e-9a1a-d079a37d218c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y Original\n",
            " 0    C\n",
            "1    C\n",
            "2    C\n",
            "3    B\n",
            "4    C\n",
            "Name: EscalaRiesgo, dtype: object\n",
            "y Encoded\n",
            "    0\n",
            "0  2\n",
            "1  2\n",
            "2  2\n",
            "3  1\n",
            "4  2\n"
          ]
        }
      ],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "\n",
        "y_encoded = pd.DataFrame(label_encoder.fit_transform(y))\n",
        "\n",
        "print(\"y Original\\n\", y.head(5))\n",
        "print(\"y Encoded\\n\", y_encoded.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9ufANFZ0_6P"
      },
      "source": [
        "### Definir CNN\n",
        "\n",
        "La red neuronal tendrá dos capas ocultas, cada una seguida de una Dropout layer para prevenir overfitting, lo cual es clave por la cantidad limitada de datos con los que se cuenta. Por esta razón, se utiliza regularización L2.\n",
        "\n",
        "Debido a que la variable dependiente es multi-clase, es necesario agregar una capa de salida de 4 unidades con una activación softmax que es la recomendada al hacer clasificaciones de este tipo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uXJppIs0_6Q"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],),\n",
        "                 kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4wHkkop0_6Q"
      },
      "source": [
        "### Entrenar modelo y hacer predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMi5mK1c0_6Q",
        "outputId": "2f09f0bc-99c4-4073-af63-7b2100b52005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.2428 - loss: 2.0150 - val_accuracy: 0.1240 - val_loss: 1.6308\n",
            "Epoch 2/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5800 - loss: 1.1534 - val_accuracy: 0.1292 - val_loss: 1.4534\n",
            "Epoch 3/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6850 - loss: 0.9361 - val_accuracy: 0.1602 - val_loss: 1.2729\n",
            "Epoch 4/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7480 - loss: 0.8099 - val_accuracy: 0.1938 - val_loss: 1.2054\n",
            "Epoch 5/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7532 - loss: 0.7249 - val_accuracy: 0.2377 - val_loss: 1.1146\n",
            "Epoch 6/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7627 - loss: 0.6708 - val_accuracy: 0.2506 - val_loss: 1.0940\n",
            "Epoch 7/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7932 - loss: 0.6071 - val_accuracy: 0.2791 - val_loss: 1.0670\n",
            "Epoch 8/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8020 - loss: 0.5729 - val_accuracy: 0.3282 - val_loss: 1.0188\n",
            "Epoch 9/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8049 - loss: 0.5685 - val_accuracy: 0.3902 - val_loss: 0.9704\n",
            "Epoch 10/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8181 - loss: 0.5388 - val_accuracy: 0.4625 - val_loss: 0.9281\n",
            "Epoch 11/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8237 - loss: 0.5141 - val_accuracy: 0.4755 - val_loss: 0.9356\n",
            "Epoch 12/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8411 - loss: 0.4626 - val_accuracy: 0.5090 - val_loss: 0.9482\n",
            "Epoch 13/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8620 - loss: 0.4477 - val_accuracy: 0.5556 - val_loss: 0.9023\n",
            "Epoch 14/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8678 - loss: 0.4239 - val_accuracy: 0.5659 - val_loss: 0.9124\n",
            "Epoch 15/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8680 - loss: 0.4274 - val_accuracy: 0.6124 - val_loss: 0.8224\n",
            "Epoch 16/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8627 - loss: 0.4305 - val_accuracy: 0.6279 - val_loss: 0.8386\n",
            "Epoch 17/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8796 - loss: 0.3783 - val_accuracy: 0.6279 - val_loss: 0.8738\n",
            "Epoch 18/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9044 - loss: 0.3445 - val_accuracy: 0.6537 - val_loss: 0.8625\n",
            "Epoch 19/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8968 - loss: 0.3739 - val_accuracy: 0.6589 - val_loss: 0.8202\n",
            "Epoch 20/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8819 - loss: 0.3604 - val_accuracy: 0.6615 - val_loss: 0.8370\n",
            "Epoch 21/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9138 - loss: 0.3349 - val_accuracy: 0.6641 - val_loss: 0.8693\n",
            "Epoch 22/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9141 - loss: 0.3017 - val_accuracy: 0.6718 - val_loss: 0.9111\n",
            "Epoch 23/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9272 - loss: 0.3065 - val_accuracy: 0.6693 - val_loss: 0.9206\n",
            "Epoch 24/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9234 - loss: 0.2820 - val_accuracy: 0.6770 - val_loss: 0.9670\n",
            "Epoch 25/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9367 - loss: 0.2730 - val_accuracy: 0.6848 - val_loss: 0.9370\n",
            "Epoch 26/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9327 - loss: 0.2659 - val_accuracy: 0.6796 - val_loss: 0.9978\n",
            "Epoch 27/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9415 - loss: 0.2451 - val_accuracy: 0.6848 - val_loss: 1.0838\n",
            "Epoch 28/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9489 - loss: 0.2161 - val_accuracy: 0.6873 - val_loss: 1.0272\n",
            "Epoch 29/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9470 - loss: 0.2270 - val_accuracy: 0.6873 - val_loss: 1.0478\n",
            "Epoch 30/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9733 - loss: 0.1821 - val_accuracy: 0.6977 - val_loss: 1.1887\n",
            "Epoch 31/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9659 - loss: 0.2059 - val_accuracy: 0.6977 - val_loss: 1.3180\n",
            "Epoch 32/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9575 - loss: 0.2097 - val_accuracy: 0.6951 - val_loss: 1.3257\n",
            "Epoch 33/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9700 - loss: 0.1708 - val_accuracy: 0.6899 - val_loss: 1.3346\n",
            "Epoch 34/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9746 - loss: 0.1662 - val_accuracy: 0.6977 - val_loss: 1.3547\n",
            "Epoch 35/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9798 - loss: 0.1548 - val_accuracy: 0.6977 - val_loss: 1.4070\n",
            "Epoch 36/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9757 - loss: 0.1593 - val_accuracy: 0.7028 - val_loss: 1.4711\n",
            "Epoch 37/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9807 - loss: 0.1530 - val_accuracy: 0.7003 - val_loss: 1.4734\n",
            "Epoch 38/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9779 - loss: 0.1591 - val_accuracy: 0.6977 - val_loss: 1.5238\n",
            "Epoch 39/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9784 - loss: 0.1577 - val_accuracy: 0.6925 - val_loss: 1.6393\n",
            "Epoch 40/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9851 - loss: 0.1452 - val_accuracy: 0.6951 - val_loss: 1.5473\n",
            "Epoch 41/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9871 - loss: 0.1355 - val_accuracy: 0.6873 - val_loss: 1.6327\n",
            "Epoch 42/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9893 - loss: 0.1297 - val_accuracy: 0.6822 - val_loss: 1.7599\n",
            "Epoch 43/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9866 - loss: 0.1302 - val_accuracy: 0.6977 - val_loss: 1.7298\n",
            "Epoch 44/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9867 - loss: 0.1324 - val_accuracy: 0.6951 - val_loss: 1.7607\n",
            "Epoch 45/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9824 - loss: 0.1457 - val_accuracy: 0.6899 - val_loss: 1.8096\n",
            "Epoch 46/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9867 - loss: 0.1220 - val_accuracy: 0.6925 - val_loss: 1.7029\n",
            "Epoch 47/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9851 - loss: 0.1217 - val_accuracy: 0.6925 - val_loss: 1.7954\n",
            "Epoch 48/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9908 - loss: 0.1193 - val_accuracy: 0.6925 - val_loss: 1.8981\n",
            "Epoch 49/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9846 - loss: 0.1323 - val_accuracy: 0.6925 - val_loss: 1.7383\n",
            "Epoch 50/50\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9880 - loss: 0.1317 - val_accuracy: 0.6899 - val_loss: 1.8127\n",
            "13/13 - 0s - 5ms/step - accuracy: 0.9830 - loss: 0.1745\n",
            "\n",
            "Test accuracy: 0.9829683899879456\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "history = model.fit(X_train_smote, y_train_smote, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"\\nTest accuracy: {test_acc}\")\n",
        "\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8mXRbvn0_6R"
      },
      "source": [
        "### Calcular métricos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ey5T3ON0_6R",
        "outputId": "20bdbfb8-d474-4d00-abe3-7a39ddf639b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall: 0.9829683698296837\n",
            "Precision: 0.9817366447645749\n",
            "F1 Score: 0.9822716017419996\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.40      0.44         5\n",
            "           1       0.98      0.97      0.98       142\n",
            "           2       0.99      1.00      1.00       264\n",
            "\n",
            "    accuracy                           0.98       411\n",
            "   macro avg       0.82      0.79      0.81       411\n",
            "weighted avg       0.98      0.98      0.98       411\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "print(f\"Precision: {precision}\")\n",
        "\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print(f\"F1 Score: {f1}\")\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWtUQG1Q0_6R"
      },
      "source": [
        "### Evaluación con Algoritmos de scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNMRHGcsO_rG"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_algorithms(X, y):\n",
        "    # Split the data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Apply SMOTE to balance the classes\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Check the class distribution\n",
        "    print(\"Original class distribution:\")\n",
        "    print(y_train.value_counts())\n",
        "    print(\"\\nBalanced class distribution:\")\n",
        "    print(y_train_balanced.value_counts())\n",
        "\n",
        "    # Define the models to be used\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs'),\n",
        "        'Decision Tree': DecisionTreeClassifier(),\n",
        "        'Support Vector Classifier': SVC(),\n",
        "        'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "        'Naive Bayes': GaussianNB(),\n",
        "        'Linear Discriminant Analysis': LinearDiscriminantAnalysis()\n",
        "    }\n",
        "\n",
        "    # Initialize a list to store the results\n",
        "    results = []\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    for name, model in models.items():\n",
        "        start_time = time.time()\n",
        "        model.fit(X_train_balanced, y_train_balanced)\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
        "        precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
        "        f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "        # Append the results\n",
        "        results.append({\n",
        "            'Model': name,\n",
        "            'Recall': recall,\n",
        "            'Precision': precision,\n",
        "            'F1 Score': f1,\n",
        "            'Training Time (s)': training_time\n",
        "        })\n",
        "\n",
        "    # Convert results to a DataFrame and display\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGWuOPaHPFS7",
        "outputId": "4c9f12b5-cc22-4216-f772-3dedc7e5bc9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original class distribution:\n",
            "EscalaRiesgo\n",
            "C    645\n",
            "B    305\n",
            "A      9\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Balanced class distribution:\n",
            "EscalaRiesgo\n",
            "C    645\n",
            "B    645\n",
            "A    645\n",
            "Name: count, dtype: int64\n",
            "                          Model    Recall  Precision  F1 Score  \\\n",
            "0           Logistic Regression  0.965937   0.967340  0.966546   \n",
            "1                 Decision Tree  0.948905   0.944883  0.946045   \n",
            "2     Support Vector Classifier  0.861314   0.853208  0.856127   \n",
            "3           K-Nearest Neighbors  0.664234   0.710413  0.674319   \n",
            "4                   Naive Bayes  0.953771   0.972422  0.962749   \n",
            "5  Linear Discriminant Analysis  0.956204   0.958340  0.956470   \n",
            "\n",
            "   Training Time (s)  \n",
            "0           0.186074  \n",
            "1           0.089533  \n",
            "2           0.122938  \n",
            "3           0.007824  \n",
            "4           0.011207  \n",
            "5           0.036557  \n"
          ]
        }
      ],
      "source": [
        "target_column = 'EscalaRiesgo'\n",
        "\n",
        "train_and_evaluate_algorithms(X, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBCZXA21nxjN"
      },
      "source": [
        "## 3. [Ajuste Fino](##AjusteFino)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxQBYeCa5LRd"
      },
      "source": [
        "De acuerdo a la métrica F1 Score, los 2 mejores modelos son:\n",
        "- Logistic Regression\n",
        "- Convolutional Neural Network (Tensorflow Keras)\n",
        "\n",
        "Con base en estos dos modelos, realizaremos el afinamiento de hiperparámetros. Definimos una función que realizará el afinamiento de hiperparámetros con base a los 2 modelos elegidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbUTJwLyECad",
        "outputId": "84422c87-d142-4df9-89f6-e7ea73c35e34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original class distribution:\n",
            "0\n",
            "2    645\n",
            "1    305\n",
            "0      9\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Balanced class distribution:\n",
            "0\n",
            "0    645\n",
            "1    645\n",
            "2    645\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "y_encoded = pd.DataFrame(label_encoder.fit_transform(y))\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply SMOTE to balance the classes\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check the class distribution\n",
        "print(\"Original class distribution:\")\n",
        "print(y_train.value_counts())\n",
        "print(\"\\nBalanced class distribution:\")\n",
        "print(y_train_balanced.value_counts())\n",
        "\n",
        "def create_keras_model(learning_rate=0.001, dropout_rate=0.3, neurons=64, X_train=None):\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(neurons, activation='relu', input_shape=(81,),\n",
        "                     kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Dense(neurons // 2, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.Dense(4, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate_algorithms(X, y):\n",
        "\n",
        "    # Initialize a list to store the results\n",
        "    results = []\n",
        "\n",
        "    # Hyperparameter tuning for Logistic Regression\n",
        "    log_reg = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
        "    log_reg_params = {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'solver': ['newton-cg', 'lbfgs', 'saga']\n",
        "    }\n",
        "    log_reg_grid = GridSearchCV(log_reg, log_reg_params, cv=5, scoring='f1_weighted')\n",
        "    start_time = time.time()\n",
        "    log_reg_grid.fit(X_train_balanced, y_train_balanced)\n",
        "    training_time = time.time() - start_time\n",
        "    log_reg_best = log_reg_grid.best_estimator_\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = log_reg_best.predict(X_test)\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
        "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
        "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "    # Append the results\n",
        "    results.append({\n",
        "        'Model': 'Logistic Regression (Tuned)',\n",
        "        'Recall': recall,\n",
        "        'Precision': precision,\n",
        "        'F1 Score': f1,\n",
        "        'Training Time (s)': training_time,\n",
        "        'Hyperparams': log_reg_grid.best_params_,\n",
        "    })\n",
        "\n",
        "\n",
        "    # Hyperparameter tuning for CNN (Tensorflow Keras)\n",
        "\n",
        "    keras_model = KerasClassifier(build_fn=create_keras_model, verbose=0)\n",
        "\n",
        "    keras_params = {\n",
        "        'batch_size': [\n",
        "            32,\n",
        "            64,\n",
        "        ],\n",
        "        'epochs': [\n",
        "            50,\n",
        "            100,\n",
        "        ],\n",
        "        'model__learning_rate': [0.001, 0.01],\n",
        "        'model__dropout_rate': [0.5, 0.7],\n",
        "        'model__neurons': [64, 128],\n",
        "    }\n",
        "\n",
        "    keras_grid = GridSearchCV(keras_model, keras_params, cv=5, scoring='f1_weighted')\n",
        "\n",
        "    start_time = time.time()\n",
        "    keras_grid.fit(X_train_balanced, y_train_balanced)\n",
        "    training_time = time.time() - start_time\n",
        "    keras_best = keras_grid.best_estimator_\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = keras_best.predict(X_test)\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
        "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
        "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "    # Append the results\n",
        "    results.append({\n",
        "        'Model': 'Keras CNN (Tuned)',\n",
        "        'Recall': recall,\n",
        "        'Precision': precision,\n",
        "        'F1 Score': f1,\n",
        "        'Training Time (s)': training_time,\n",
        "        'Hyperparams': keras_grid.best_params_,\n",
        "    })\n",
        "\n",
        "    # Convert results to a DataFrame and display\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oxQEgMK7Eio",
        "outputId": "9b5992cb-d8fa-4b20-bbe0-645380e06de4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         Model    Recall  Precision  F1 Score  \\\n",
            "0  Logistic Regression (Tuned)  0.961071   0.963240  0.961846   \n",
            "1            Keras CNN (Tuned)  0.931873   0.931062  0.931383   \n",
            "\n",
            "   Training Time (s)  \\\n",
            "0          45.390329   \n",
            "1         524.141762   \n",
            "\n",
            "                                                                                                           Hyperparams  \n",
            "0                                                                                         {'C': 10, 'solver': 'lbfgs'}  \n",
            "1  {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.5, 'model__learning_rate': 0.001, 'model__neurons': 128}  \n"
          ]
        }
      ],
      "source": [
        "train_and_evaluate_algorithms(X, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1GWTatNECae"
      },
      "source": [
        "### Resumen de Modelos\n",
        "\n",
        "<center>\n",
        "\n",
        "| Modelo | Recall | Precision | F1 Score |\n",
        "| --- | --- | --- | --- |\n",
        "| Logistic Regression | 0.9659 | 0.9673 | 0.9665 |\n",
        "| Decision Tree | 0.9489 | 0.9448 | 0.9460 |\n",
        "| Support Vector Classifier | 0.8613 | 0.8532 | 0.8561 |\n",
        "| K-Nearest Neighbors | 0.6642 | 0.7103 | 0.6743 |\n",
        "| Naive Bayes | 0.9537 | 0.9724 | 0.9627 |\n",
        "| Linear Discriminant Analysis | 0.9562 | 0.9583 | 0.9564 |\n",
        "| Tensorflow Keras CNN | 0.9829 | 0.9817 | 0.9822 |\n",
        "| Logistic Regression (Tuned) | 0.9610 | 0.9632 | 0.9618 |\n",
        "| Tensorflow Keras CNN (Tuned )| 0.9318 | 0.9310 | 0.9313 |\n",
        "\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL5uzaHXECae"
      },
      "source": [
        "# VI. Modelos de Ensamble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akEbug_GECae"
      },
      "source": [
        "## Homogéneos y Heterogéneos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uY3KjP8_ECae"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_algorithms(X, y):\n",
        "    # Split the data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Apply SMOTE to balance the classes\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Check the class distribution\n",
        "    print(\"Original class distribution:\")\n",
        "    print(y_train.value_counts())\n",
        "    print(\"\\nBalanced class distribution:\")\n",
        "    print(y_train_balanced.value_counts())\n",
        "\n",
        "    nb = GaussianNB()\n",
        "    ld = LinearDiscriminantAnalysis()\n",
        "    dt = DecisionTreeClassifier()\n",
        "    lr = LogisticRegression(C=10, solver=\"lbfgs\",  multi_class=\"multinomial\")\n",
        "\n",
        "    # Define the models to be used\n",
        "    models = {\n",
        "        \"Logistic Regression (Tuned)\": lr,\n",
        "        \"Random Forest\": RandomForestClassifier(),\n",
        "        \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "        \"Voting\": VotingClassifier(estimators=[(\"nb\", nb), (\"ld\", ld), (\"dt\", dt)], voting=\"hard\"),\n",
        "        \"Stacking\": StackingClassifier(estimators=[(\"nb\", nb), (\"ld\", ld), (\"dt\", dt)], final_estimator=lr),\n",
        "    }\n",
        "\n",
        "    # Initialize a list to store the results\n",
        "    results = []\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    for name, model in models.items():\n",
        "        start_time = time.time()\n",
        "        model.fit(X_train_balanced, y_train_balanced)\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
        "        precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
        "        f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "        # Append the results\n",
        "        results.append({\n",
        "            'Model': name,\n",
        "            'Recall': recall,\n",
        "            'Precision': precision,\n",
        "            'F1 Score': f1,\n",
        "            'Training Time (s)': training_time\n",
        "        })\n",
        "\n",
        "    # Convert results to a DataFrame and display\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "s1eqvvCqECaf",
        "outputId": "988b04b6-5a22-4b85-b13c-0b7c60b30aed"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9ed15421b577>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_evaluate_algorithms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "train_and_evaluate_algorithms(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGJXJC4SECaf"
      },
      "source": [
        "## Comparativa de Modelo Individual vs Ensamble\n",
        "\n",
        "\n",
        "<center>\n",
        "\n",
        "| Modelo | Recall | Precision | F1 Score | Training Time (s) |\n",
        "| --- | --- | --- | --- | --- |\n",
        "| Voting | 0.9756 | 0.9688 | 0.9721 | 0.0818 |\n",
        "| Gradient Boosting | 0.9756 | 0.9722 | 0.9731 | 3.9130 |\n",
        "| Logistic Regression (Tuned) | 0.9610 | 0.9625 | 0.9614 | 0.1877 |\n",
        "| Random Forest | 0.9367 | 0.9260 | 0.9312 | 0.6112 |\n",
        "| Stacking | 0.9440 | 0.9386 | 0.9391 | 0.5552 |\n",
        "\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQPJMShnECaf"
      },
      "source": [
        "## Selección de Modelo Final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74CceqaNECaf"
      },
      "source": [
        "### De los cinco modelos Voting y Gradient Boosting fueron los que mejor desempeño tuvieron, en ambos casos las métricas de recal, precisión y F1 Score fueron las más altas. En específico Recall y F1 Score, en precisión voting estuvo ligeramente abajo. Sin embargo, en términos de training time, voting fue mucho mejor que Gradient Boosting. Derivado de lo anterior, elegimos voting como el mejor módelo debido a que en términos de rendimiento es el mejor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vWl3WquECag"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxGjfnvSECag"
      },
      "source": [
        "## Gráficos de Modelo Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pw-JT8_iECag"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "        print(f\"\\nConfusion Matrix for {name}:\\n\", cm)\n",
        "\n",
        "        # Calculate ROC and AUC\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            y_prob = model.predict_proba(X_test)\n",
        "            fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1], pos_label=model.classes_[1])\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            plt.figure()\n",
        "            plt.plot(fpr, tpr, color='blue', lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "            plt.xlim([0.0, 1.0])\n",
        "            plt.ylim([0.0, 1.05])\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title(f'ROC Curve - {name}')\n",
        "            plt.legend(loc=\"lower right\")\n",
        "            plt.show()\n",
        "\n",
        "    # Plot results\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Performance metrics\n",
        "    results_df.plot(x='Model', y=['Recall', 'Precision', 'F1 Score'], kind='bar', ax=ax[0])\n",
        "    ax[0].set_title('Performance Metrics by Model')\n",
        "    ax[0].set_xlabel('Model')\n",
        "    ax[0].set_ylabel('Score')\n",
        "    ax[0].legend(loc='best')\n",
        "\n",
        "    # Training time\n",
        "    results_df.plot(x='Model', y='Training Time (s)', kind='bar', color='skyblue', ax=ax[1])\n",
        "    ax[1].set_title('Training Time by Model')\n",
        "    ax[1].set_xlabel('Model')\n",
        "    ax[1].set_ylabel('Training Time (s)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRYlWDsQVQqR"
      },
      "source": [
        "# Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzBGo875mKam"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8JbQ5JB6XJa_"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}